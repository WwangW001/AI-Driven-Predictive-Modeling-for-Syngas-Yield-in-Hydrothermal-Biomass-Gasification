{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecc3774-6eb8-470c-823a-09bce03a3de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import shap\n",
    "import matplotlib\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVR\n",
    "from IPython.display import display, HTML\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.interpolate import make_interp_spline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16177d4-7624-4a36-a5e2-093c0efcb624",
   "metadata": {},
   "outputs": [],
   "source": [
    "Syngas_data=pd.read_csv('./Paper_1_own')  \n",
    "Syngas_data=Syngas_data.iloc[:, 2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a660f-c37a-4c84-a510-60104c4c3e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Paper_1_corr_matrix = Syngas_data.corr()  \n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(Paper_1_corr_matrix,annot=True,cmap='coolwarm',fmt='.2f',linewidths=.5)\n",
    "plt.title('Pearson correlation heatmap of input features and syngas experimental results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8278702a-93e0-4e7a-a54c-d23d31f1d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns = ['C (%)', 'H (%)', 'O (%)', 'N (%)','S(%)', 'Ash (%)', 'BTW(ratio)', 'Particle size(mm)','Temp. C', 'P (MPa)', 'Res- Time (min)']  # 选择数据，并将其分成x，y\n",
    "Y_columns = ['H2 mol/kg', 'CO2 mol/kg', 'CO mol/kg', 'CH4 mol/kg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56078430-a4be-4c7c-90e6-5987532f54d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Syngas_data[X_columns]\n",
    "Y = Syngas_data[Y_columns]\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "scaler_Y = StandardScaler()\n",
    "\n",
    "features = scaler_X.fit_transform(X)\n",
    "labels = scaler_Y.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5da4666-0631-4599-99c8-d9d43004b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.Tensor(features)\n",
    "        self.labels = torch.Tensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286fa927-a0c2-43cb-9336-c47bd68b7b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2)\n",
    "features_test, features_dev, labels_test, labels_dev = train_test_split(features_test, labels_test, test_size=0.15)\n",
    "train_dataset = CustomDataset(features_train, labels_train)\n",
    "test_dataset = CustomDataset(features_test, labels_test)\n",
    "dev_dataset = CustomDataset(features_dev, labels_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b47d0c-f692-4db6-bb40-49ca5d00be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, weight_decay=0.0):\n",
    "        super(ANeuralNetwork, self).__init__()\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            if i == 0:\n",
    "                layer = nn.Linear(input_size, hidden_sizes[i])\n",
    "            else:\n",
    "                layer = nn.Linear(hidden_sizes[i - 1], hidden_sizes[i])\n",
    "            self.hidden_layers.append(layer)\n",
    "            self.hidden_layers.append(nn.Tanh())\n",
    "        self.output_layer = nn.Linear(hidden_sizes[-1] if hidden_sizes else input_size, output_size)\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = F.relu(layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bdcf05-406d-4f55-a6e5-53b308b2a420",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,drop_last=False)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c821c84f-9be1-479a-a9b6-140ea95e72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataloader, input_size, hidden_sizes, output_size, num_epochs=10, learning_rate=0.001, device='cpu',weight_decay=0.01):\n",
    "    train_losses = []\n",
    "    model = ANeuralNetwork(input_size, hidden_sizes, output_size,weight_decay=weight_decay)\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        average_loss = epoch_loss / len(train_dataloader)\n",
    "        train_losses.append(average_loss)\n",
    "        if epoch % 20 == 0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5541943d-c990-4801-a87a-6e89d095ccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "trained_model = train_model(train_dataloader, input_size=11, hidden_sizes=[16,8], output_size=4, num_epochs=1000, learning_rate=0.038, device=device,weight_decay=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29821583-97b6-42dc-8bc5-13e275f179f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trained_model.state_dict(), 'ANN.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30ded40-401c-45bb-9e3c-600caa3b2cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"ANN.pth\"\n",
    "model_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "print(\"Layer Sizes:\")\n",
    "for key in model_dict.keys():\n",
    "    if \"weight\" in key or \"bias\" in key: \n",
    "        param_tensor = model_dict[key]\n",
    "        print(f\"{key}: {param_tensor.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7903dc22-8444-44ee-ad47-3bcb33c23be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_model = ANeuralNetwork(input_size=11, hidden_sizes=[16,8], output_size=4, weight_decay=0.015)\n",
    "ANN_model.load_state_dict(torch.load('ANN.pth', map_location=torch.device('cpu')))\n",
    "ANN_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e117878b-9dd2-4605-bd39-c485fb735b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.hidden_layers = model.hidden_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for layer in self.hidden_layers:\n",
    "            x = F.tanh(layer(x))\n",
    "            features.append(x)\n",
    "        return torch.cat(features, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00761fc8-44d6-45e4-a64d-891920f327a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_evaluation_mode(model):\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ec0431-5b8e-46ec-911e-80e3569ae224",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor(ANN_model)\n",
    "feature_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef7a79-c6ac-4db4-b5b9-ce97306d4120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, data_loader, device='cpu'):\n",
    "    features = []\n",
    "    labels = []\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            if isinstance(outputs, list):\n",
    "                for output in outputs:\n",
    "                    features.append(output.view(output.size(0), -1))\n",
    "            else:\n",
    "                features.append(outputs.view(outputs.size(0), -1))\n",
    "            labels.append(targets)\n",
    "    return torch.cat(features), torch.cat(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbf0b69-1d16-4d19-b639-378df0995f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features, Y_train = extract_features(feature_extractor, train_dataloader)\n",
    "X_test_features, Y_test = extract_features(feature_extractor, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21d49e4-9977-4c18-a2b2-4ca8726fef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_svm_params(X_train, Y_train, param_grid, cv=5, device='cuda:0'):\n",
    "    X_train = X_train.to(device)\n",
    "    Y_train = Y_train.to(device)\n",
    "    \n",
    "    base_svr_model = SVR()\n",
    "\n",
    "    svm_model = make_pipeline(StandardScaler(), MultiOutputRegressor(base_svr_model))\n",
    "    grid_search = GridSearchCV(svm_model, param_grid, scoring='neg_mean_squared_error', cv=cv)\n",
    "    grid_search.fit(X_train.numpy(), Y_train.numpy())\n",
    "    best_params = grid_search.best_params_\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860c8c81-3869-41ae-a66e-5d9ee04c1267",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'multioutputregressor__estimator__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'multioutputregressor__estimator__C': [0.1, 1.0, 10.0],\n",
    "    'multioutputregressor__estimator__epsilon': [0.01, 0.1, 0.2,0.6,0.9,1.2],\n",
    "    'multioutputregressor__estimator__gamma': ['scale', 'auto', 0.1, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eeb288-5e4a-4d64-a7b7-98916d475217",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features, Y_train = extract_features(feature_extractor, train_dataloader)\n",
    "best_params = find_best_svm_params(X_train_features, Y_train, param_grid, device='cpu')\n",
    "\n",
    "print(\"best_params:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289b2db1-b3af-4bd7-ab38-a2a208c71e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_kernel = 'rbf'\n",
    "new_C = 10\n",
    "new_epsilon = 0.1\n",
    "new_gamma = 'scale'\n",
    "\n",
    "new_svr_model = SVR(kernel=new_kernel, C=new_C, epsilon=new_epsilon, gamma=new_gamma)\n",
    "svm_model = make_pipeline(StandardScaler(), MultiOutputRegressor(new_svr_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf7de6-5907-47c6-96fe-588bf97f4a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model.fit(X_train_features.numpy(), Y_train.numpy())\n",
    "svm_predictions = svm_model.predict(X_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a39e2c-c0d6-4a52-a0eb-49fe7538dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(ANN_model.state_dict(), 'ANN-SVM_ann.pth')\n",
    "# joblib.dump(svm_model, 'ANN-SVM_svm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf57d05-cf39-46d4-9033-076312197109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_svm_model = joblib.load('ANN-SVM_svm.pkl')\n",
    "# loaded_svm_model = make_pipeline(StandardScaler(), MultiOutputRegressor(loaded_svm_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e5f408-7327-41d0-9ca3-6043cce1db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ANN_model = ANeuralNetwork(input_size=11, hidden_sizes=[16,8], output_size=4, weight_decay=0.005)\n",
    "trained_ANN_model.load_state_dict(torch.load('ANN-SVM_ann.pth', map_location=torch.device('cpu')))\n",
    "trained_SVR_model = joblib.load('ANN-SVM_svm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e721a9-0714-4a46-9210-829801624d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_model_predict(trained_ann,trained_svm,features_train): \n",
    "    feature_extractor_own = FeatureExtractor(trained_ann)    \n",
    "    features_train = torch.Tensor(features_train)\n",
    "    X_features =  feature_extractor_own(features_train)\n",
    "    predictions = trained_svm.predict(X_features.detach().numpy())\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf0d2eb-0db3-42a0-a6e7-7b93e72e9b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_model(FEATURE):\n",
    "    predictions_train_own = []\n",
    "    predictions_batch = combined_model_predict(trained_ANN_model, trained_SVR_model, FEATURE)\n",
    "    predictions_train_own.append(predictions_batch)\n",
    "    return predictions_train_own[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6713453-deb0-40fb-822b-8c4cb4175c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(combined_model, features_train)\n",
    "shap_values = explainer.shap_values(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d11f155-5e11-4119-967b-3e9136bca32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for output_idx in range(4):\n",
    "    shap_vals_output = [shap_values[:,:,output_idx]]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8)) \n",
    "    \n",
    "    shap.summary_plot(shap_vals_output, features_train, feature_names=X_columns, show=False, \n",
    "                      color=matplotlib.cm.Oranges.reversed(), color_bar=False)\n",
    "    \n",
    "    plt.title(Y_columns[output_idx])  \n",
    "    plt.gca().get_legend().remove()  \n",
    "    # plt.savefig(f'{output_idx}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22532b4f-9232-4b24-a881-ba5ef0431d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_outputs = 4\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "for output_idx in range(num_outputs):\n",
    "\n",
    "    shap_abs_sum_per_feature = np.sum(np.abs(shap_values[:, :, output_idx]), axis=0)\n",
    "    total_shap_abs_sum = np.sum(shap_abs_sum_per_feature)\n",
    "    feature_proportions = shap_abs_sum_per_feature / total_shap_abs_sum\n",
    "\n",
    "    row = output_idx // 2\n",
    "    col = output_idx % 2\n",
    "    ax = axs[row, col]\n",
    "    ax.barh(X_columns, feature_proportions)\n",
    "    ax.set_title(f'PDP of {Y_columns[output_idx]} for Each Feature')\n",
    "    ax.set_xlabel('Proportion')\n",
    "\n",
    "    np.savetxt(f'X_columns_output_{output_idx + 1}.txt', X_columns, fmt='%s', delimiter='\\t', header='X Columns')\n",
    "    np.savetxt(f'feature_proportions_output_{output_idx + 1}.txt', feature_proportions, fmt='%.18e', delimiter='\\t', header='Feature Proportions')\n",
    "\n",
    "    print(f'{Y_columns[output_idx]}:')\n",
    "    print(f'  Biomass Element: {np.sum(feature_proportions[0:6])}')\n",
    "    print(f'  Gasification Conditions: {np.sum(feature_proportions[6:])}')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('PDP.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0104a7c-722d-4d47-afe8-6c7abb76c228",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_3d = np.transpose(shap_values, (2, 0, 1))  # (n_classes, n_samples, n_features)\n",
    "\n",
    "for i in range(len(Y_columns)):\n",
    "\n",
    "    shap_df = pd.DataFrame(shap_values_3d[i], columns=X_columns)\n",
    "    shap_df['Sample Index'] = range(shap_values_3d[i].shape[0])  \n",
    "\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    shap.summary_plot(shap_values_3d[i], features_train, plot_type=\"dot\", show=False, feature_names=X_columns)\n",
    "\n",
    "    # plt.title(Y_columns[i], fontsize=20)\n",
    "    plt.xlabel(\"SHAP Value\", fontsize=18)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1445108-6c01-4793-9b81-ca96298f44a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_filename(filename):\n",
    "    return re.sub(r'[^\\w\\s-]', '', filename).strip()\n",
    "\n",
    "\n",
    "\n",
    "shap_values_3d = np.transpose(shap_values, (2, 0, 1))\n",
    "explainer = shap.Explainer(combined_model, features_train)\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "\n",
    "for i in range(len(Y_columns)):\n",
    "    plt.figure(figsize=(20, 15)) \n",
    "    shap.summary_plot(shap_values_3d[i], features_train, plot_type=\"dot\", show=False, feature_names=X_columns)\n",
    "\n",
    "    # plt.title(Y_columns[i], fontdict={'fontsize': 20})\n",
    "    plt.xlabel(\"SHAP Value\", fontdict={'fontsize': 18})\n",
    "    # plt.ylabel(\"Feature\")\n",
    "    cleaned_filename = clean_filename(Y_columns[i])\n",
    "\n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a61143-d584-4c8d-b49c-7dd193f55dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import UnivariateSpline\n",
    "original_feature_values = scaler_X .inverse_transform(features_train)\n",
    "\n",
    "def clean_filename(filename):\n",
    "    return re.sub(r'[^\\w\\s-]', '', filename).strip()\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "shap_values_3d = np.transpose(shap_values, (2, 0, 1)) \n",
    "\n",
    "\n",
    "# selected_features = [\"Temp. C\", \"H (%)\", \"BTW(ratio)\", \"N (%)\"]\n",
    "# X_columns = ['C (%)', 'H (%)', 'O (%)', 'N (%)','S(%)', 'Ash (%)', 'BTW(ratio)', 'Particle size(mm)','Temp. C', 'P (MPa)', 'Res- Time (min)'] \n",
    "selected_features = ['BTW(ratio)']\n",
    "selected_indices = [X_columns.index(feature) for feature in selected_features]\n",
    "\n",
    "# print(f\"Selected indices: {selected_indices}\")\n",
    "# print(f\"X_columns: {X_columns}\")\n",
    "# print(f\"shap_values_3d shape: {shap_values_3d.shape}\")\n",
    "\n",
    "fig, axs = plt.subplots(1, len(selected_features), figsize=(5, 4))\n",
    "# axs_flat = axs.flatten()\n",
    "\n",
    "for i, feature_index in enumerate(selected_indices):\n",
    "    feature_name = X_columns[feature_index]\n",
    "    \n",
    "\n",
    "    shap_values_for_feature = shap_values_3d[0][:, feature_index]\n",
    "    feature_values = original_feature_values[:, feature_index]\n",
    "    \n",
    "    axs.scatter(feature_values, shap_values_for_feature, s=10, color='blue')\n",
    "    unique_feature_values, unique_indices = np.unique(feature_values, return_index=True)\n",
    "    unique_shap_values_for_feature = shap_values_for_feature[unique_indices]\n",
    "    \n",
    "    if len(unique_feature_values) > 1:\n",
    "        sort_idx = np.argsort(unique_feature_values)\n",
    "        spl = UnivariateSpline(unique_feature_values[sort_idx], unique_shap_values_for_feature[sort_idx], s=0.5, k=3)\n",
    "        x_smooth = np.linspace(unique_feature_values.min(), unique_feature_values.max(), 300)\n",
    "        y_smooth = spl(x_smooth)\n",
    "        axs.plot(x_smooth, y_smooth, 'r--')\n",
    "    # axs[i].set_title(feature_name)\n",
    "    axs.set_xlabel(feature_name)\n",
    "    axs.set_ylabel(f'SHAP Value for {feature_name}')\n",
    "\n",
    "    cleaned_filename = clean_filename(X_columns[i])\n",
    "    # plt.savefig('others.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d3ce98-8e40-43c5-8cf2-6d52307deb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_columns = ['C (%)', 'H (%)', 'O (%)', 'N (%)','S(%)', 'Ash (%)', 'BTW(ratio)', 'Particle size(mm)','Temp. C', 'P (MPa)', 'Res- Time (min)'] \n",
    "original_feature_values = scaler_X .inverse_transform(features_train)\n",
    "feature_index = 6 \n",
    "data = {\n",
    "    'Temp. C':original_feature_values[:, feature_index],\n",
    "    'SHAP_values': shap_values_3d[0][:, feature_index]  \n",
    "}\n",
    "\n",
    "num_bins = 10\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df['Temp. C_bins'] = pd.cut(df['Temp. C'], bins=num_bins, labels=False)\n",
    "\n",
    "\n",
    "average_shap = df.groupby('Temp. C_bins')['SHAP_values'].mean().reset_index()\n",
    "\n",
    "\n",
    "print(average_shap)\n",
    "original_feature_values[:, feature_index]\n",
    "min_number = np.min(original_feature_values[:, feature_index])\n",
    "max_number = np.max(original_feature_values[:, feature_index])\n",
    "number_points = 5\n",
    "X_number = np.linspace(min_number, max_number, number_points )\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(average_shap['Temp. C_bins'], average_shap['SHAP_values'], marker='o', linestyle='--', color='r')\n",
    "plt.title('Average SHAP Values per Temp. C')\n",
    "plt.xlabel('Temp. C')\n",
    "plt.ylabel('Average SHAP Value')\n",
    "plt.grid()\n",
    "plt.xticks(range(num_bins))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39660832-e942-455d-9d1d-64deacbfe12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_columns = ['C (%)', 'H (%)', 'O (%)', 'N (%)','S(%)', 'Ash (%)', 'BTW(ratio)', 'Particle size(mm)','Temp. C', 'P (MPa)', 'Res- Time (min)'] \n",
    "input_feature_index_1 = 8  \n",
    "input_feature_index_2 = 7 \n",
    "output_index = 3 \n",
    "\n",
    "\n",
    "shap_values_input_output = shap_values[:, [input_feature_index_1, input_feature_index_2], output_index]\n",
    "\n",
    "min_value_1 = np.min(features_train[:, input_feature_index_1])\n",
    "max_value_1 = np.max(features_train[:, input_feature_index_1])\n",
    "min_value_2 = np.min(features_train[:, input_feature_index_2])\n",
    "max_value_2 = np.max(features_train[:, input_feature_index_2])\n",
    "num_points = shap_values.shape[0] \n",
    "\n",
    "feature_values_1 = np.linspace(min_value_1, max_value_1, num_points)\n",
    "feature_values_2 = np.linspace(min_value_2, max_value_2, num_points)\n",
    "\n",
    "\n",
    "partial_dependence_values = np.zeros((len(feature_values_1), len(feature_values_2)))\n",
    "for i, value_1 in enumerate(feature_values_1):\n",
    "    for j, value_2 in enumerate(feature_values_2):\n",
    "        input_feature = features_train.copy()\n",
    "        input_feature[:, input_feature_index_1] = value_1\n",
    "        input_feature[:, input_feature_index_2] = value_2\n",
    "        prediction = combined_model(input_feature)\n",
    "        partial_dependence_values[i, j] = prediction[0, output_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da46153a-d1d4-455f-833e-cca7c0e93800",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_syn_1 = np.min(Syngas_data.iloc[:, input_feature_index_1])\n",
    "max_syn_1 = np.max(Syngas_data.iloc[:, input_feature_index_1])\n",
    "min_syn_2 = np.min(Syngas_data.iloc[:, input_feature_index_2])\n",
    "max_syn_2 = np.max(Syngas_data.iloc[:, input_feature_index_2])\n",
    "num_points = shap_values.shape[0]  \n",
    "\n",
    "feature_values_1_syn = np.linspace(min_syn_1, max_syn_1 , num_points)\n",
    "feature_values_2_syn = np.linspace(min_syn_2, max_syn_2, num_points)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X, Y = np.meshgrid(feature_values_2_syn, feature_values_1_syn)\n",
    "surf = ax.plot_surface(X, Y, partial_dependence_values, cmap='viridis')\n",
    "\n",
    "ax.set_xlabel(f'{X_columns[input_feature_index_2]}')\n",
    "ax.set_ylabel(f'{X_columns[input_feature_index_1]}')\n",
    "ax.set_zlabel('Two-Way Partial Dependence')\n",
    "ax.view_init(elev=50, azim=260)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809bf889-e2a5-49e3-833e-d6779c878de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X, Y = np.meshgrid(feature_values_2, feature_values_1)\n",
    "surf = ax.plot_surface(X, Y, partial_dependence_values, cmap='viridis')\n",
    "\n",
    "ax.set_xlabel(f'{X_columns[input_feature_index_2]}')\n",
    "ax.set_ylabel(f'{X_columns[input_feature_index_1]}')\n",
    "ax.set_zlabel('Two-Way Partial Dependence')\n",
    "\n",
    "ax.view_init(elev=30, azim=240)\n",
    "\n",
    "plt.title(f'Two-way partial dependance plots for {X_columns[input_feature_index_1]} and {X_columns[input_feature_index_2]} on {Y_columns[output_index]}')\n",
    "plt.show("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd2f45c-3229-43ee-a4b6-403c54a21752",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_values_1_original = scaler_X.inverse_transform(np.tile(feature_values_1[:,np.newaxis],(1, 11)))\n",
    "feature_values_1_ori = feature_values_1_original[:, input_feature_index_1]  \n",
    "\n",
    "feature_values_2_original = scaler_X.inverse_transform(np.tile(feature_values_2[:,np.newaxis],(1, 11)))\n",
    "feature_values_2_ori = feature_values_2_original[:, input_feature_index_2]\n",
    "\n",
    "# data_to_save = np.column_stack((feature_values_1_ori.repeat(len(feature_values_2_ori)), \n",
    "#                                 np.tile(feature_values_2_ori, len(feature_values_1_ori)), \n",
    "#                                 partial_dependence_values.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e72b83-faab-4045-acc9-720b12773b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
